{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial Simon Tamayo GIT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import math\n",
    "import pandas as pd\n",
    "np.set_printoptions(precision=4, suppress=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load initlial matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adjacency matrix is:\n",
      "[[0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 1 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0]\n",
      " [1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 1 0 1 0 0 1 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1]]\n",
      "Reward matrix is:\n",
      "[[ -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1]\n",
      " [ -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1]\n",
      " [ -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1]\n",
      " [ -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1]\n",
      " [ -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1]\n",
      " [ -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1]\n",
      " [ -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1]\n",
      " [ -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1]\n",
      " [ -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1]\n",
      " [ -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1]\n",
      " [ -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1]\n",
      " [ -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1]\n",
      " [ -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1]\n",
      " [ -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1]\n",
      " [ -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1]\n",
      " [ -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1]\n",
      " [ -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1 100]\n",
      " [ -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1  -1 100]]\n"
     ]
    }
   ],
   "source": [
    "# Adjacency matrix (the graph of the problem)\n",
    "adj_mat = pd.read_csv('./adjacency_matrix.csv',sep=';',index_col=0).values\n",
    "print('Adjacency matrix is:')\n",
    "print(adj_mat)\n",
    "\n",
    "# Reward matrix\n",
    "reward_mat = pd.read_csv('./reward_matrix.csv',sep=';',index_col=0).values\n",
    "print('Reward matrix is:')\n",
    "print(reward_mat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Q matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q matrix (initial) is :\n",
      "[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "# The Q matrix is the memory of the learner and it maps actions to states\n",
    "nbOfStates = len(reward_mat)\n",
    "states = np.arange(nbOfStates)\n",
    "Q = np.zeros([nbOfStates,nbOfStates])\n",
    "print('Q matrix (initial) is :')\n",
    "print(Q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q Learning algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.   0.16 0.   0.   0.   0.1  0.   0.   0.   0.   0.   0.   0.   0.\n",
      "  0.   0.   0.   0.  ]\n",
      " [0.13 0.   0.2  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "  0.   0.   0.   0.  ]\n",
      " [0.   0.16 0.   0.16 0.   0.   0.25 0.   0.   0.   0.   0.   0.   0.\n",
      "  0.   0.   0.   0.  ]\n",
      " [0.   0.   0.2  0.   0.13 0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "  0.   0.   0.   0.  ]\n",
      " [0.   0.   0.   0.16 0.   0.   0.   0.1  0.   0.   0.   0.   0.   0.\n",
      "  0.   0.   0.   0.  ]\n",
      " [0.13 0.   0.   0.   0.   0.   0.   0.   0.08 0.   0.   0.   0.   0.\n",
      "  0.   0.   0.   0.  ]\n",
      " [0.   0.   0.2  0.   0.   0.   0.   0.   0.   0.32 0.   0.   0.   0.\n",
      "  0.   0.   0.   0.  ]\n",
      " [0.   0.   0.   0.   0.13 0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "  0.   0.   0.   0.  ]\n",
      " [0.   0.   0.   0.   0.   0.1  0.   0.   0.   0.   0.   0.   0.   0.\n",
      "  0.   0.   0.   0.  ]\n",
      " [0.   0.   0.   0.   0.   0.   0.25 0.   0.   0.   0.4  0.   0.   0.\n",
      "  0.   0.   0.   0.  ]\n",
      " [0.   0.   0.   0.   0.   0.   0.   0.   0.   0.32 0.   0.51 0.   0.\n",
      "  0.25 0.   0.   0.  ]\n",
      " [0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.4  0.   0.64 0.\n",
      "  0.   0.   0.   0.  ]\n",
      " [0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.51 0.   0.\n",
      "  0.   0.   0.8  0.  ]\n",
      " [0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "  0.25 0.   0.   0.  ]\n",
      " [0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.2\n",
      "  0.   0.32 0.   0.  ]\n",
      " [0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.4  0.   0.   0.\n",
      "  0.25 0.   0.   0.  ]\n",
      " [0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.64 0.\n",
      "  0.   0.   0.   1.  ]\n",
      " [0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "  0.   0.   0.8  1.  ]]\n"
     ]
    }
   ],
   "source": [
    "# alpha = learning rate\n",
    "alpha = 1\n",
    "# gamma = discount factor\n",
    "gamma = 0.8\n",
    "\n",
    "# Training iterations\n",
    "# s = current state ; a = action ; snext = next state\n",
    "for i in range(10000):\n",
    "    s = np.random.randint(0,nbOfStates)        \n",
    "    a = np.random.choice(states[adj_mat[s]==1])\n",
    "    snext = a\n",
    "    Q[s,a]=(1-alpha)*Q[s,a] + (alpha)*(reward_mat[s,a]+gamma*(np.max(Q[snext,:])))\n",
    "\n",
    "# Normalize the matrix and remove impossible transitions\n",
    "# print(Q)\n",
    "Q = Q/np.max(Q)*adj_mat\n",
    "print(np.round(Q,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In room A go to B\n",
      "In room B go to C\n",
      "In room C go to G\n",
      "In room D go to C\n",
      "In room E go to D\n",
      "In room F go to A\n",
      "In room G go to J\n",
      "In room H go to E\n",
      "In room I go to F\n",
      "In room J go to K\n",
      "In room K go to L\n",
      "In room L go to M\n",
      "In room M go to Q\n",
      "In room N go to O\n",
      "In room O go to P\n",
      "In room P go to K\n",
      "In room Q go to EX\n",
      "In room EX go to EX\n"
     ]
    }
   ],
   "source": [
    "# The optimal policy is...\n",
    "sln = np.argmax(Q,axis=1)\n",
    "\n",
    "names_dict = {0:'A', 1:'B', 2:'C', 3:'D', 4:'E', 5:'F', 6:'G', 7:'H',\n",
    "              8:'I', 9:'J', 10:'K', 11:'L', 12:'M', 13:'N', 14:'O', 15:'P',\n",
    "              16:'Q', 17:'EX'}\n",
    "\n",
    "sln_letters = np.vectorize(names_dict.get)(sln)\n",
    "\n",
    "for i in range(nbOfStates):\n",
    "    print('In room '+ names_dict[i]+ ' go to '+sln_letters[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
